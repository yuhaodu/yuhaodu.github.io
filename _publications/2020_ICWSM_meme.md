---
title: "Understanding Visual Memes: an Empirical Analysis of Text Superimposed on Memes Shared on Twitter"
category: articles
permalink: /publication/2020_ICWSM_meme.md/
excerpt: "Visual memes have become an important mechanism through which ideologically potent and hateful content spreads on today's social media platforms.  At the same time, they are also a mechanism through which we convey much more mundane things, like pictures of cats with strange accents.  Little is known, however, about the relative percentage of visual memes shared by real people that fall into these, or other, thematic categories.  The present work focuses on visual memes that contain superimposed text. We carry out the first large-scale study on the themes contained in the text of these memes, which we refer to as \emph{image-with-text} memes.  We find that 30\% of the image-with-text memes  in our sample which have identifiable themes are politically relevant, and that these politically relevant memes are shared more often by Democrats than Republicans. We also find disparities in who expresses themselves via image-with-text memes, and images in general, versus other forms of expression on Twitter. The fact that some individuals use images with text to express themselves, instead of sending a plain text tweet, suggests potential consequences for the representativeness of analyses that ignore text contained in images.  "
venue: "Proceedings of the International AAAI Conference on Web and Social Media"
date: 2020-05-26
---

<a href="https://stuartgeiger.com/papers/gigo-fat2020.pdf">Download PDF here</a>.

Abstract: Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available.  We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a ``gold standard'' of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.


Recommended citation: R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang. 2020. "Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?" In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 18 pages. https://stuartgeiger.com/papers/gigo-fat2020.pdf https://doi.org/10.1145/3351095.3372862 Number 1." <i>Journal 1</i>. 1(1).
